{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = \"/Users/jonathanlin/Documents/GitHub/research_transfer/\"\n",
    "csv_path = cwd + \"datasets/Animal_Kingdom/action_recognition/annotation/val.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from typing import Any, Type\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from torchmetrics import Accuracy\n",
    "import torch.utils.data as data\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.nn import functional as F\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(\"/Users/jonathanlin/Documents/GitHub/research_transfer\")\n",
    "from data_tools import ak_ar_images\n",
    "from data_tools.ak_ar_images import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/Users/jonathanlin/Documents/GitHub/research_transfer/datasets/Animal_Kingdom/action_recognition/annotation/df_action.xlsx\")\n",
    "landmarks_frame = pd.read_csv(csv_path, delimiter = \" \")\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# caption_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "#     \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | unfrozen | Sequential | 27.7 M\n",
      "1 | sigm     | Sigmoid    | 0     \n",
      "----------------------------------------\n",
      "27.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.7 M    Total params\n",
      "110.823   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408f9224e2b24343b4de6452bd4d9035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_tools/ak_ar_images/converted.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m model \u001b[39m=\u001b[39m clip2_baseline(num_classes \u001b[39m=\u001b[39m \u001b[39m46\u001b[39m)\n\u001b[1;32m     69\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(max_epochs \u001b[39m=\u001b[39m epochs, fast_dev_run\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 70\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:529\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    527\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    528\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 529\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    530\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    531\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:568\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    559\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    560\u001b[0m )\n\u001b[1;32m    562\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    563\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    564\u001b[0m     ckpt_path,\n\u001b[1;32m    565\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    566\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    567\u001b[0m )\n\u001b[0;32m--> 568\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    570\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    970\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    975\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1014\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1013\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1014\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1015\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1016\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1043\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1040\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1042\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1045\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1047\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:98\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@_no_grad_context\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[_OUT_DICT]:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_data()\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip:\n\u001b[1;32m    100\u001b[0m         \u001b[39mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:150\u001b[0m, in \u001b[0;36m_EvaluationLoop.setup_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m stage \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m source \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_source\n\u001b[0;32m--> 150\u001b[0m dataloaders \u001b[39m=\u001b[39m _request_dataloader(source)\n\u001b[1;32m    151\u001b[0m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mstage\u001b[39m.\u001b[39mdataloader_prefix\u001b[39m}\u001b[39;00m\u001b[39m_dataloader()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dataloaders, CombinedLoader):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:330\u001b[0m, in \u001b[0;36m_request_dataloader\u001b[0;34m(data_source)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \n\u001b[1;32m    322\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39m    The requested dataloader\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[39m\"\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[1;32m    326\u001b[0m     \u001b[39m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[39m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[39m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[39m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m     \u001b[39mreturn\u001b[39;00m data_source\u001b[39m.\u001b[39;49mdataloader()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:297\u001b[0m, in \u001b[0;36m_DataLoaderSource.dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the dataloader from the source.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[39mIf the source is a module, the method with the corresponding :attr:`name` gets called.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[0;32m--> 297\u001b[0m     \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minstance\u001b[39m.\u001b[39;49mtrainer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, pl_module\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minstance)\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance, pl\u001b[39m.\u001b[39mLightningDataModule):\n\u001b[1;32m    299\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance\u001b[39m.\u001b[39mtrainer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:144\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    143\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    147\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m, in \u001b[0;36mclip2_baseline.val_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mval_dataloader\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 60\u001b[0m     train_loader, val_loader \u001b[39m=\u001b[39m dataloader\u001b[39m.\u001b[39;49mget_data()\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m val_loader\n",
      "File \u001b[0;32m~/Documents/GitHub/research_transfer/data_tools/ak_ar_images/dataloader.py:79\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(batch_size, num_workers)\u001b[0m\n\u001b[1;32m     76\u001b[0m cwd \u001b[39m=\u001b[39m cwd[\u001b[39m0\u001b[39m:cwd\u001b[39m.\u001b[39mrfind(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m     77\u001b[0m \u001b[39m# set the root directory of the project to 2 layers above the current dataloader\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mdata_tools/ak_ar_images/converted.json\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     80\u001b[0m data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m     81\u001b[0m f\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_tools/ak_ar_images/converted.json'"
     ]
    }
   ],
   "source": [
    "class clip2_baseline(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # frozen model\n",
    "        # backbone = models.resnet50(weights=\"DEFAULT\")\n",
    "        # backbone.fc = nn.Sequential(\n",
    "        #     nn.Dropout(p = 0.2),\n",
    "        #     nn.Linear(backbone.fc.in_features, num_classes * 16)\n",
    "        # )\n",
    "        self.backbone = processor\n",
    "\n",
    "        # unfrozen model\n",
    "        self.unfrozen = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, num_classes * 4),\n",
    "            nn.Dropout(p = 0.2),\n",
    "            nn.Linear(num_classes * 4, num_classes)\n",
    "        )\n",
    "\n",
    "        self.sigm = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # self.backbone.eval()\n",
    "        # with torch.no_grad():\n",
    "        x = self.backbone(images=x, return_tensors=\"pt\").to(torch.float32)\n",
    "        x = x[\"pixel_values\"].squeeze()\n",
    "        return self.sigm(self.unfrozen(x))\n",
    "        # return self.sigm(self.backbone(x))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        # images = images.reshape(-1, 32 * 32)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = self(images)\n",
    "        # loss = nn.BCEWithLogitsLoss()\n",
    "        loss = nn.BCEWithLogitsLoss()(outputs, labels.type(torch.float32))\n",
    "\n",
    "        print(loss)\n",
    "        return {'loss':loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        # images = images.reshape(-1, 32 * 32)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = self(images)\n",
    "        loss = nn.BCEWithLogitsLoss()(outputs, labels.type(torch.float32))\n",
    "\n",
    "        return {'loss':loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr = 0.001)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader, val_loader = dataloader.get_data()\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        train_loader, val_loader = dataloader.get_data()\n",
    "        return val_loader\n",
    "\n",
    "# f = open(\"/Users/jonathanlin/Documents/GitHub/research_transfer/data_tools/ak_ar_images/converted.json\", \"r\")\n",
    "# data = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "epochs = 10\n",
    "model = clip2_baseline(num_classes = 46)\n",
    "trainer = Trainer(max_epochs = epochs, fast_dev_run=False)\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
