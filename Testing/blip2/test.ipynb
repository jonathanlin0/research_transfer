{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = \"/Users/jonathanlin/Documents/GitHub/research_transfer/\"\n",
    "csv_path = cwd + \"datasets/Animal_Kingdom/action_recognition/annotation/val.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from typing import Any, Type\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from torchmetrics import Accuracy\n",
    "import torch.utils.data as data\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.nn import functional as F\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(\"/Users/jonathanlin/Documents/GitHub/research_transfer\")\n",
    "from data_tools import ak_ar_images\n",
    "from data_tools.ak_ar_images import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/Users/jonathanlin/Documents/GitHub/research_transfer/datasets/Animal_Kingdom/action_recognition/annotation/df_action.xlsx\")\n",
    "landmarks_frame = pd.read_csv(csv_path, delimiter = \" \")\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# caption_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "#     \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | unfrozen | Sequential | 27.7 M\n",
      "1 | sigm     | Sigmoid    | 0     \n",
      "----------------------------------------\n",
      "27.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.7 M    Total params\n",
      "110.823   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294c598b8e54443985e8c4009e55e489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1575d8940>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1436, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "_pickle.UnpicklingError: pickle data was truncated\n"
     ]
    }
   ],
   "source": [
    "class clip2_baseline(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # frozen model\n",
    "        self.backbone = processor\n",
    "\n",
    "        # unfrozen model\n",
    "        self.unfrozen = nn.Sequential(\n",
    "            nn.Linear(3 * 224 * 224, num_classes * 4),\n",
    "            nn.Dropout(p = 0.2),\n",
    "            nn.Linear(num_classes * 4, num_classes)\n",
    "        )\n",
    "\n",
    "        self.sigm = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # self.backbone.eval()\n",
    "        # with torch.no_grad():\n",
    "        output = self.backbone(images=x, return_tensors=\"pt\").to(torch.float32)\n",
    "        output = torch.squeeze(output[\"pixel_values\"])\n",
    "        # x = x.to('mps')\n",
    "        \n",
    "        items = output.size(dim=0)\n",
    "        rest = output.size(dim=1) * output.size(dim=2) * output.size(dim=3)\n",
    "        # mps_device = torch.device(\"mps\")\n",
    "        # x.to(mps_device)\n",
    "        output = output.type_as(x)\n",
    "        print(x.device)\n",
    "        \n",
    "        return self.sigm(self.unfrozen(x))\n",
    "        # return self.sigm(self.backbone(x))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        # images = images.reshape(-1, 32 * 32)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = self(images)\n",
    "        # loss = nn.BCEWithLogitsLoss()\n",
    "        loss = nn.BCELoss()(outputs, labels.type(torch.float32))\n",
    "\n",
    "        print(loss)\n",
    "        return {'loss':loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        # images = images.reshape(-1, 32 * 32)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = self(images)\n",
    "        loss = nn.BCELoss()(outputs, labels.type(torch.float32))\n",
    "\n",
    "        return {'loss':loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr = 0.001)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader, val_loader = dataloader.get_data()\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        train_loader, val_loader = dataloader.get_data()\n",
    "        return val_loader\n",
    "\n",
    "# f = open(\"/Users/jonathanlin/Documents/GitHub/research_transfer/data_tools/ak_ar_images/converted.json\", \"r\")\n",
    "# data = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# train_loader, val_loader = dataloader.get_data()\n",
    "\n",
    "epochs = 10\n",
    "model = clip2_baseline(num_classes = 46)\n",
    "trainer = Trainer(max_epochs = epochs, fast_dev_run=False)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
